# -*- org-return-follows-link: t; -*-
#+TITLE: container.sharing.io
#+AUTHOR: Hippie Hacker
#+EMAIL: hh@ii.coop
#+DATE: 28th of Augest, 2023
#+PROPERTY: header-args:bash+ :results replace verbatim code output
#+PROPERTY: header-args:bash+ :var SPACE_TLD=(symbol-value 'space-domain)
#+NOPROPERTY: header-args:bash+ :var TSOCKET=(symbol-value 'tmux-socket)
#+PROPERTY: header-args:bash+ :dir (symbol-value 'tramp-dir)
#+PROPERTY: header-args:bash+ :wrap example
#+PROPERTY: header-args:bash+ :async
#+PROPERTY: header-args:shell+ :results replace verbatim code output
#+PROPERTY: header-args:shell+ :var SPACEDOMAIN=(symbol-value 'space-domain)
#+PROPERTY: header-args:shell+ :var KUBECONFIG=(concat (getenv "HOME") "/.kube/config-" space-domain)
#+NOPROPERTY: header-args:shell+ :var TSOCKET=(symbol-value 'tmux-socket)
#+PROPERTY: header-args:shell+ :async
#+NOPROPERTY: header-args:tmux+ :session "io:ssh"
#+NOPROPERTY: header-args:tmux+ :socket (symbol-value 'tmux-socket)
#+NOSTARTUP: content
#+NOSTARTUP: overview
#+NOSTARTUP: show2levels
#+STARTUP: showeverything
* WWW
** Who?
kubermanic.com  / ii.nz team
** What?
Configure an Equinix Provided [[https://deploy.equinix.com/developers/docs/metal/hardware/standard-servers/#m3largex86][m3.large.x86]] into a single node kubernetes cluster.
** Why?
We want to serve up fast and local services for our cloud native friends in Europe.
** Where?
Frankfurt [[https://deploy.equinix.com/developers/docs/metal/locations/metros/#europe-and-middle-east][Equinix Facility]]
** How?
Deploying Ubuntu + Kubernetes + flux + gitops + coder + some templates.
* Inventory
** Computer with Ubuntu
In this case we created one using the Equinix [[https://deploy.equinix.com/developers/docs/metal/libraries/cli/][metal cli]].
*** device creation
:PROPERTIES:
:header-args:shell+: :var KUBECONFIG="/Users/hh/.kube/container.sharing.io"
:header-args:shell+: :var CODER_CONFIG_DIR="/Users/hh/.config/coder.container.sharing.io"
:header-args:tmux+: :session ":creation"
:END:
**** metal org
#+begin_src shell :sync
metal project get
#+end_src

#+RESULTS:
#+begin_example
+--------------------------------------+------------+----------------------+
|                  ID                  |    NAME    |       CREATED        |
+--------------------------------------+------------+----------------------+
| 82b5c425-8dd4-429e-ae0d-d32f265c63e4 | sharing.io | 2023-06-16T19:54:45Z |
+--------------------------------------+------------+----------------------+
#+end_example

**** create
#+begin_src shell :wrap "src yaml" :sync
metal device create \
    --metro fr \
    --plan m3.large.x86 \
    --operating-system ubuntu_22_04 \
    --hostname cp1.container.sharing.io \
    --output yaml \
    cp1.container.sharing.io
#+end_src

#+RESULTS:
#+begin_src yaml
billing_cycle: hourly
created_at: "2023-09-11T12:05:46Z"
created_by:
  avatar_thumb_url: https://www.gravatar.com/avatar/88099dd30103c7cc4ab013e4aa9b2c83?d=mm
  created_at: "2017-08-15T16:22:11Z"
  email: packet@cncf.io
  first_name: CNCF
  full_name: CNCF Team
  href: /metal/v1/users/19d434d9-ca19-4230-928e-ea3115a3e859
  id: 19d434d9-ca19-4230-928e-ea3115a3e859
  last_name: Team
  level: verified
  short_id: 19d434d9
  updated_at: "2023-09-11T12:05:47Z"
facility:
  address:
    address: Kruppstrasse 121-127
    city: Frankfurt
    coordinates: {}
    country: DE
    id: cff635b5-5ea2-4750-86b6-1f9b9b2a9e74
    zip_code: "60388"
  code: fr2
  features:
  - baremetal
  - backend_transfer
  - layer_2
  - global_ipv4
  - ibx
  id: 3576541d-3278-4bad-ba07-4a88ce986cad
  metro:
    code: fr
    country: DE
    id: b1ac82b2-616c-4405-9424-457ef6edf9ae
    name: Frankfurt
  name: Frankfurt
hostname: cp1.container.sharing.io
href: /metal/v1/devices/fbfcffec-0bf2-4983-9af2-33a1717bf592
id: fbfcffec-0bf2-4983-9af2-33a1717bf592
ip_addresses: []
metro:
  code: fr
  country: DE
  id: b1ac82b2-616c-4405-9424-457ef6edf9ae
  name: Frankfurt
network_ports:
- bond:
    id: 0e4b4c43-9245-4f94-893f-1843e27bf5b5
    name: bond0
  data:
    bonded: true
    mac: b4:96:91:f8:f1:f6
  disbond_operation_supported: true
  href: /metal/v1/ports/4e694186-f099-4268-90ee-9e3f9e2e3ea7
  id: 4e694186-f099-4268-90ee-9e3f9e2e3ea7
  name: eth0
  type: NetworkPort
- bond:
    id: 0e4b4c43-9245-4f94-893f-1843e27bf5b5
    name: bond0
  data:
    bonded: true
    mac: b4:96:91:f8:f1:f7
  disbond_operation_supported: true
  href: /metal/v1/ports/5fdefe46-ce47-49c7-81f4-8fd9d225989e
  id: 5fdefe46-ce47-49c7-81f4-8fd9d225989e
  name: eth1
  type: NetworkPort
- data:
    bonded: true
  disbond_operation_supported: true
  href: /metal/v1/ports/0e4b4c43-9245-4f94-893f-1843e27bf5b5
  id: 0e4b4c43-9245-4f94-893f-1843e27bf5b5
  name: bond0
  network_type: layer2-bonded
  type: NetworkBondPort
operating_system:
  distro: ubuntu
  name: Ubuntu 22.04 LTS
  provisionable_on:
  - a3.large.opt-m3a2
  - a3.large.opt-m3a2.x86
  - a3.large.opt-m3a4
  - a3.large.opt-m3a4.x86
  - a3.large.opt-s4a1
  - a3.large.opt-s4a1.x86
  - a3.large.opt-s5a1
  - a3.large.opt-s5a1.x86
  - a3.large.opt-s6a1
  - a3.large.opt-s6a1.x86
  - a3.large.x86
  - c2.medium.x86
  - c3.large.arm64
  - c3.medium.opt-c1
  - c3.medium.opt-c1.x86
  - c3.medium.opt-c1m1
  - c3.medium.opt-c1m1.x86
  - c3.medium.x86
  - c3.small.x86
  - f3.large.x86
  - f3.medium.x86
  - m2.xlarge.x86
  - m3.large.opt-c2
  - m3.large.opt-c2.x86
  - m3.large.opt-c2m3s3
  - m3.large.opt-c2m3s3.x86
  - m3.large.opt-c2m4
  - m3.large.opt-c2m4.x86
  - m3.large.x86
  - m3.small.opt-m1
  - m3.small.opt-m1.x86
  - m3.small.x86
  - n2.xlarge.x86
  - n3.xlarge.opt-m3s0
  - n3.xlarge.opt-m3s0.x86
  - n3.xlarge.opt-m3s2.x86
  - n3.xlarge.opt-m4
  - n3.xlarge.opt-m4.x86
  - n3.xlarge.opt-m4s2.x86
  - n3.xlarge.x86
  - npi.testing
  - nvidia3.a100.medium
  - nvidia3.a30.large
  - nvidia3.a30.medium
  - nvidia3.a40.medium
  - nvidia3.h100.large.x86
  - s3.xlarge.x86
  - w2.intel.5120.768.30960
  - w2.intel.5120.768.30960.x86
  - w3aintel.8358.512.6080
  - w3aintel.8358.512.6080.x86
  - w3amd.7402p.256.8160
  - w3amd.7402p.256.8160.x86
  - w3amd.7502p.1024.23520
  - w3amd.7502p.1024.23520.x86
  - w3amd.7502p.512.8160
  - w3amd.7502p.512.8160.x86
  - w3amd.7513p.1024.15840
  - w3amd.7513p.1024.15840.x86
  - w3amd.7513p.1024.8160
  - w3amd.7513p.1024.8160.x86
  - w3tintel.8358.512.6080
  - w3tintel.8358.512.6080.x86
  - x2.xlarge.x86
  - x.large.arm
  slug: ubuntu_22_04
  version: "22.04"
plan:
  available_in:
  - href: /metal/v1/facilities/cbb790c1-812f-462a-8597-7229f35dc995
    id: ""
  - href: /metal/v1/facilities/c84253c1-6f7e-4de6-9a46-77e60616135f
    id: ""
  - href: /metal/v1/facilities/30ad1492-f715-4cb0-9b1d-bb20718dd87f
    id: ""
  - href: /metal/v1/facilities/387c1ab8-3cc3-408e-8a96-93a873cc1e50
    id: ""
  - href: /metal/v1/facilities/d37a89ba-5d74-4ea6-b30c-3464b4b20683
    id: ""
  - href: /metal/v1/facilities/3576541d-3278-4bad-ba07-4a88ce986cad
    id: ""
  - href: /metal/v1/facilities/4a84b014-90db-48a1-823b-c061430bf0c6
    id: ""
  - href: /metal/v1/facilities/0711ea9a-9132-42a6-a390-ba22b522873e
    id: ""
  - href: /metal/v1/facilities/1c4455e6-c7cf-4cc5-a386-fe52ffc57ae7
    id: ""
  - href: /metal/v1/facilities/8002e6d0-737a-47c5-85a1-092fedced9f1
    id: ""
  - href: /metal/v1/facilities/134b9e4c-5830-45f9-89ce-3757853fcf77
    id: ""
  - href: /metal/v1/facilities/1b0be1cf-47ae-4406-8ac8-44379cd50b4c
    id: ""
  - href: /metal/v1/facilities/86940551-4551-4407-bd66-37db1a8eb507
    id: ""
  - href: /metal/v1/facilities/20f3bf20-58dd-467d-bfaa-c5e0afd4b1b2
    id: ""
  - href: /metal/v1/facilities/cbdaaaa4-64e4-40fc-b75f-428938ea85ce
    id: ""
  - href: /metal/v1/facilities/6307e264-cb42-4996-b4b7-150f66793938
    id: ""
  - href: /metal/v1/facilities/3b095113-a2c7-4fe9-bda7-2e7e78d96a1f
    id: ""
  - href: /metal/v1/facilities/f3d69797-5401-4f5b-a54e-6f09c8393c0e
    id: ""
  - href: /metal/v1/facilities/917c709b-bcd9-4878-9be3-0ce5a071e4a8
    id: ""
  - href: /metal/v1/facilities/ec0d7b59-549b-4252-8177-f66a0da052a9
    id: ""
  - href: /metal/v1/facilities/353a2477-ce18-42d4-aab9-ad53f7a82fee
    id: ""
  - href: /metal/v1/facilities/917e9941-8323-487f-8688-2b0b11baacd4
    id: ""
  - href: /metal/v1/facilities/d2a72094-26c9-4372-8d65-051424bc370a
    id: ""
  - href: /metal/v1/facilities/b96126f9-156a-43cd-a044-997082ec07eb
    id: ""
  - href: /metal/v1/facilities/d2244149-b87a-4934-b102-b826e403ca31
    id: ""
  - href: /metal/v1/facilities/12b2c338-d11c-4fae-869d-79f07f73bd82
    id: ""
  available_in_metros:
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  - id: ""
  class: m3.large.x86
  deployment_types:
  - on_demand
  - spot_market
  description: Our m3.large.x86 server is ideal for virtualization, with an AMD EPYC
    7502P 32C/64T processor @ 2.5GHz and 256GB RAM
  id: cb6a01ea-0120-4a59-ad6a-bcc14d8bf487
  line: baremetal
  name: m3.large.x86
  pricing:
    hour: 3.1
  reservation_pricing:
    Metros:
      am:
        one_year:
          month: 1737.4
        three_year:
          month: 1241
      ch:
        one_year:
          month: 1584.1
        three_year:
          month: 1131.5
      da:
        one_year:
          month: 1584.1
        three_year:
          month: 1131.5
      dc:
        one_year:
          month: 1584.1
        three_year:
          month: 1131.5
      fr:
        one_year:
          month: 1737.4
        three_year:
          month: 1241
      hk:
        one_year:
          month: 1890.7
        three_year:
          month: 1350.5
      la:
        one_year:
          month: 1584.1
        three_year:
          month: 1131.5
      ld:
        one_year:
          month: 1737.4
        three_year:
          month: 1241
      md:
        one_year:
          month: 1737.4
        three_year:
          month: 1241
      ny:
        one_year:
          month: 1584.1
        three_year:
          month: 1131.5
      one_month:
        one_year: null
        three_year: null
      pa:
        one_year:
          month: 1737.4
        three_year:
          month: 1241
      se:
        one_year:
          month: 1584.1
        three_year:
          month: 1131.5
      sg:
        one_year:
          month: 1890.7
        three_year:
          month: 1350.5
      sl:
        one_year:
          month: 1890.7
        three_year:
          month: 1350.5
      sp:
        one_year:
          month: 2376.15
        three_year:
          month: 1697.25
      sv:
        one_year:
          month: 1584.1
        three_year:
          month: 1131.5
      sy:
        one_year:
          month: 1890.7
        three_year:
          month: 1350.5
      tr:
        one_year:
          month: 1737.4
        three_year:
          month: 1241
      ty:
        one_year:
          month: 1890.7
        three_year:
          month: 1350.5
    one_year:
      month: 1584.1
    three_year:
      month: 1131.5
  slug: m3.large.x86
  specs:
    cpus:
    - count: 1
      type: AMD EPYC 7513 32-Core Processor @ 2.6Ghz
    drives:
    - count: 2
      size: 240GB
      type: SSD
    - count: 2
      size: 3.8TB
      type: NVME
    features:
      raid: true
      txt: true
    memory:
      total: 256GB
    nics:
    - count: 2
      type: 25Gbps
project:
  backend_transfer_enabled: false
  href: /metal/v1/projects/82b5c425-8dd4-429e-ae0d-d32f265c63e4
  id: ""
  organization:
    address:
      address: ""
      country: ""
      zip_code: ""
    id: ""
    primary_owner:
      id: ""
      short_id: ""
  payment_method:
    billing_address: {}
    id: ""
    organization:
      address:
        address: ""
        country: ""
        zip_code: ""
      id: ""
      primary_owner:
        id: ""
        short_id: ""
provisioning_events:
- body: Provisioning started
  interpolated: Provisioning started
  type: provisioning.101
- body: Network configured
  interpolated: Network configured
  type: provisioning.102
- body: Configuration written, restarting device
  interpolated: Configuration written, restarting device
  type: provisioning.103
- body: Connected to magic install system
  interpolated: Connected to magic install system
  type: provisioning.104
- body: OS image retrieved
  interpolated: OS image retrieved
  type: provisioning.104.50
- body: Server partitions created
  interpolated: Server partitions created
  type: provisioning.105
- body: Operating system packages installed
  interpolated: Operating system packages installed
  type: provisioning.106
- body: Server networking interfaces configured
  interpolated: Server networking interfaces configured
  type: provisioning.107
- body: Cloud-init packages installed and configured
  interpolated: Cloud-init packages installed and configured
  type: provisioning.108
- body: Installation finished, rebooting server
  interpolated: Installation finished, rebooting server
  type: provisioning.109
- body: Device phoned home and is ready to go
  interpolated: Device phoned home and is ready to go
  type: provisioning.110
short_id: fbfcffec
ssh_keys:
- Owner:
    href: ""
  created_at: ""
  fingerprint: ""
  href: /metal/v1/ssh-keys/e9dc6064-3de8-49ea-9eed-4ab422c38458
  id: ""
  key: ""
  label: ""
  updated_at: ""
- Owner:
    href: ""
  created_at: ""
  fingerprint: ""
  href: /metal/v1/ssh-keys/88d645f5-c6c2-47bc-b23e-991e92e8753c
  id: ""
  key: ""
  label: ""
  updated_at: ""
- Owner:
    href: ""
  created_at: ""
  fingerprint: ""
  href: /metal/v1/ssh-keys/f445dcb8-6b19-44af-865f-bd5be5fcf37c
  id: ""
  key: ""
  label: ""
  updated_at: ""
- Owner:
    href: ""
  created_at: ""
  fingerprint: ""
  href: /metal/v1/ssh-keys/dbfd215e-cf88-4bff-96ce-62375d45ea79
  id: ""
  key: ""
  label: ""
  updated_at: ""
- Owner:
    href: ""
  created_at: ""
  fingerprint: ""
  href: /metal/v1/ssh-keys/93e15f6b-d45a-4704-9732-4748ad13eae6
  id: ""
  key: ""
  label: ""
  updated_at: ""
- Owner:
    href: ""
  created_at: ""
  fingerprint: ""
  href: /metal/v1/ssh-keys/bfa3dbf1-1f48-41b1-9e29-e3ceda891232
  id: ""
  key: ""
  label: ""
  updated_at: ""
- Owner:
    href: ""
  created_at: ""
  fingerprint: ""
  href: /metal/v1/ssh-keys/60f04f19-f108-4a26-aed2-624c18cf5b7d
  id: ""
  key: ""
  label: ""
  updated_at: ""
- Owner:
    href: ""
  created_at: ""
  fingerprint: ""
  href: /metal/v1/ssh-keys/5c5df043-14a8-4bb0-98dc-0b3a0b4b8224
  id: ""
  key: ""
  label: ""
  updated_at: ""
- Owner:
    href: ""
  created_at: ""
  fingerprint: ""
  href: /metal/v1/ssh-keys/7fb2fe6f-7146-4f18-ac10-6a6ed62be1d5
  id: ""
  key: ""
  label: ""
  updated_at: ""
- Owner:
    href: ""
  created_at: ""
  fingerprint: ""
  href: /metal/v1/ssh-keys/8d892ec0-a6d0-4f2b-825e-f695eafda7fd
  id: ""
  key: ""
  label: ""
  updated_at: ""
- Owner:
    href: ""
  created_at: ""
  fingerprint: ""
  href: /metal/v1/ssh-keys/2b2e94f7-b372-4e00-a9e2-a853ea689ff2
  id: ""
  key: ""
  label: ""
  updated_at: ""
- Owner:
    href: ""
  created_at: ""
  fingerprint: ""
  href: /metal/v1/ssh-keys/027c5898-fd71-4e2e-8190-2a7a8f575e63
  id: ""
  key: ""
  label: ""
  updated_at: ""
state: queued
switch_uuid: "27503866"
updated_at: "2023-09-11T12:05:47Z"
user: root
volumes: []

#+end_src

**** ip
#+name: ip
#+begin_src shell :sync
metal device get --filter hostname=cp1.container.sharing.io -o json | jq '.[0].ip_addresses[0].address' -r
#+end_src

#+RESULTS: ip
#+begin_example
136.144.49.79
#+end_example
**** id
#+name: id
#+begin_src shell :sync
metal device get --filter hostname=cp1.container.sharing.io -o json | jq -r '.[0].id'
#+end_src

#+RESULTS: id
#+begin_example
eb8a09d1-5c5d-45bc-9961-e04ac23f9e4b
#+end_example
**** facility
#+name: facility
#+begin_src shell :sync
metal device get --filter hostname=cp1.container.sharing.io -o json | jq -r '.[0].facility.code'
#+end_src

#+RESULTS: facility
#+begin_example
fr2
#+end_example

**** device
#+name: id
#+begin_src shell :sync
metal device get
#+end_src

#+RESULTS: id
#+begin_example
+--------------------------------------+----------------+-------------+--------+----------------------+
|                  ID                  |    HOSTNAME    |     OS      | STATE  |       CREATED        |
+--------------------------------------+----------------+-------------+--------+----------------------+
| 905bd051-e623-4f82-997f-b7d6b352d218 | nix.sharing.io | NixOS 23.05 | active | 2023-09-09T08:32:11Z |
+--------------------------------------+----------------+-------------+--------+----------------------+
#+end_example
**** serial bios / hardware console access
:PROPERTIES:
:header-args:shell+: :var ID=eval-block("id")
:header-args:shell+: :var FACILITY=eval-block("facility")
# :header-args:tmux+: :prologue (concat "ID=" eval-block("id") "\nFACILITY=" eval-block("facility") "\n")
:END:

This will allow us to watch for any errors that come up, but usually is uneventful, just giving us an indecation of how far along we are.
#+name: ssh_sos
#+begin_src shell :var ID=eval-block("id") :var FACILITY=eval-block("facility") :wrap "src tmux :session \":ssh\"" :sync
echo ssh $ID@sos.$FACILITY.platformequinix.com
#+end_src

#+RESULTS: ssh_sos
#+begin_src tmux :session ":ssh"
ssh afbf4b93-a21d-4581-8cc4-7b3327acb141@sos.ld7.platformequinix.com
#+end_src


** Domain
NS Records and a separate tsig key are configured on powerdns.ii.nz
#+name: spacedomain
#+begin_src bash :wrap "example" :sync :cache yes
echo -n $SPACE_TLD
#+end_src

#+RESULTS[3e90bcba2041cbd062d8f655fe5492b84380ff86]: spacedomain
#+begin_example
container.sharing.io
#+end_example

* Connect domain to our IP
** What is our IP?
#+begin_src shell :sync
metal device get --filter hostname=cp1.container.sharing.io -o json | jq '.[0].ip_addresses[0].address' -r
#+end_src

#+RESULTS:
#+begin_example
136.144.49.79
#+end_example

** Add DOMAIN -> Address pointing our IP
We did this via the GUI, but here is the verified result.
#+name: add main A record
#+begin_src shell :sync
dig A $SPACE_TLD +short @ns.ii.nz
#+end_src

#+RESULTS: add main A record
#+begin_example
145.40.113.253
#+end_example

** Add *.DOMAIN -> Address pointing to our IP
#+name: add wildcard A record
#+begin_src shell :sync
dig A random123.$SPACE_TLD +short
#+end_src

#+RESULTS: add wildcard A record
#+begin_example
145.40.113.253
#+end_example
* Verify DNS + SSH Connectivity
** ssh root@container.sharing.io
You should be able to login with your password (or ssh key)
#+begin_src tmux :prologue (concat "export SPACE_TLD=" space-domain "\n")
ssh root@136.144.49.79
#+end_src
** ssh-import-id to ensure Hippie, Stephen, and Zach Have access
#+begin_src tmux
ssh-import-id gh:hh gh:iiamabby gh:heyste gh:zachmandeville gh:iiamabby
#+end_src
* install
** trust packages from google, kubernetes, and docker
#+begin_src tmux
curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg \
    | gpg --dearmor -o /etc/apt/trusted.gpg.d/google.gpg
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key \
    | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/kubernetes-apt-keyring.gpg
curl -fsSL https://download.docker.com/linux/ubuntu/gpg \
    | gpg --dearmor -o /etc/apt/trusted.gpg.d/docker.gpg
#+end_src
** add repos from docker and googl
#+begin_src tmux
apt-add-repository "deb https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /" -y
add-apt-repository "deb https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" -y
apt-add-repository "deb http://packages.cloud.google.com/apt cloud-sdk main" -y
#+end_src
** ttyd tmux curl containerd
#+begin_src tmux
DEBIAN_FRONTENT=noninteractive apt-get install -y \
    -o Dpkg::Options::="--force-confdef" \
    -o Dpkg::Options::="--force-confold"  \
    ttyd \
    tmux \
    kitty-terminfo \
    containerd.io \
    curl \
    docker-ce \
    docker-ce-cli \
    kubelet \
    kubeadm \
    open-iscsi \
    nfs-common
#+end_src

** cilium
#+begin_src tmux
sudo su -
cd /tmp
curl -L --remote-name-all \
    https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-amd64.tar.gz.sha256sum
tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin
rm cilium-linux-amd64.tar.gz cilium-linux-amd64.tar.gz.sha256sum
#+end_src
** flux
#+begin_src tmux
curl -s https://fluxcd.io/install.sh | bash
#+end_src
** helm
#+begin_src tmux
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
#+end_src
* configure starting kubernetes
** swap
*** disable swap
#+begin_src tmux
sudo swapoff -a
#+end_src
*** remove swap from /etc/fstab
Swap will be remounted when we reboot, unless we remove it from the File System TAB.
#+begin_src tmux
sudo sed -i '/swap/d' /etc/fstab
#+end_src
*** Check results
Swap will be remounted when we reboot, unless we remove it from the File System TAB.
#+begin_src tmux
free -m
cat /etc/fstab
#+end_src
** containerd
Kubernetes needs systemdcgroup when using cilium
*** [[/ssh:root@uk.sharing.io:/etc/containerd/config.toml][/etc/containerd/config.toml]]
#+begin_src toml :tangle (concat tramp-dir "etc/containerd/config.toml")
version = 2
[plugins]
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
    runtime_type = "io.containerd.runc.v2"
    [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
      SystemdCgroup = true
#+end_src
*** restart containerd w/ new config
#+begin_src bash :results silent :sync
sudo systemctl restart containerd
#+end_src
** [[/ssh:root@uk.sharing.io:/etc/crictl.yaml][/etc/crictl.yaml]]
crictl needs to be confugured to use our containred socket. (It complains otherwise)
#+begin_src toml :tangle (concat tramp-dir "etc/crictl.yaml")
runtime-endpoint: unix:///var/run/containerd/containerd.sock
image-endpoint: unix:///var/run/containerd/containerd.sock
timeout: 10
debug: false
#+end_src
** [[/ssh:root@uk.sharing.io:/etc/kubernetes/kubeadm-config.yaml][/etc/kubernetes/kubeadm-config.yaml]]
*** Default Config
#+begin_src bash :wrap "src yaml" :sync
kubeadm config print init-defaults
#+end_src

#+RESULTS:
#+begin_src yaml
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  name: node
  taints: null
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.k8s.io
kind: ClusterConfiguration
kubernetesVersion: 1.28.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
#+end_src

*** My InitConfiguration
We need to disabled kube-proxy, and ensure we use the criSocket.
We will let cilium handle the kube-proxy aspects of the cluster
#+begin_src toml :tangle (concat tramp-dir "etc/kubernetes/kubeadm-config.yaml") :comments no
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
skipPhases:
  - addon/kube-proxy
nodeRegistration:
  taints: []
#+end_src
*** My ClusterConfiguration
Let's be sure our naming is specific to this cluster for Certs and DNS
#+begin_src toml :tangle (concat tramp-dir "etc/kubernetes/kubeadm-config.yaml") :comments no
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
clusterName: container.sharing.io
kubernetesVersion: 1.28.1
controlPlaneEndpoint: "k8s.container.sharing.io:6443"
apiServer:
  certSans:
    - "136.144.49.79"
    - "k8s.container.sharing.io"
#+end_src
** [[/ssh:root@uk.sharing.io:/etc/kubernetes/cilium-values.yaml][/etc/kubernetes/cilium-values.yaml]]
These are the helm chart values for the 'kubeproxy-free' setup of Cilium
- [[https://docs.cilium.io/en/latest/network/kubernetes/kubeproxy-free/#quick-start][KubeProxy free Quickstart]]
- [[https://github.com/cilium/cilium/tree/v1.13.3/install/kubernetes/cilium#values][Cilium Helm Values Documentation]]
*** base config
#+begin_src yaml :tangle (concat tramp-dir "etc/kubernetes/cilium-values.yaml")
k8sServiceHost: k8s.container.sharing.io
k8sServicePort: 6443
kubeProxyReplacement: strict
policyEnforcementMode: "never"
operator:
  replicas: 1
#+end_src
*** Enable Gateway API
I hear this is cool
#+begin_src yaml :tangle (concat tramp-dir "etc/kubernetes/cilium-values.yaml")
gatewayAPI:
  enabled: true
#+end_src
*** (dis)able IngressController
I'm really keen to try this out, but we need to find a way to set the following on the cilium-ingress:
#+begin_src yaml
externalIPs:
  - 136.144.49.79
loadBalancerIP: 136.144.49.79
#+end_src
Along with figuring out connectivity.
#+begin_src yaml :tangle (concat tramp-dir "etc/kubernetes/cilium-values.yaml")
ingressController:
  enabled: false
  service:
    # type: NodePort
    type: LoadBalancer
#+end_src
*** hubble
#+begin_src yaml :tangle (concat tramp-dir "etc/kubernetes/cilium-values.yaml")
hubble:
  enabled: true
  listenAddress: ":4244"
  metrics:
    enabled:
      - dns
      - drop
      - tcp
      - flow
      - port-distribution
      - icmp
      - http
  relay:
    enabled: true
  ui:
    enabled: true
#+end_src
* configure completion
#+begin_src tmux
helm completion bash > /etc/bash_completion.d/helm
flux completion bash > /etc/bash_completion.d/flux
kubectl completion bash > /etc/bash_completion.d/kubectl
#+end_src
* actually init and start kubernetes
** Pull down kubernetes container images
#+begin_src bash :sync
kubeadm config images pull
#+end_src

#+RESULTS:
#+begin_example
[config/images] Pulled registry.k8s.io/kube-apiserver:v1.28.1
[config/images] Pulled registry.k8s.io/kube-controller-manager:v1.28.1
[config/images] Pulled registry.k8s.io/kube-scheduler:v1.28.1
[config/images] Pulled registry.k8s.io/kube-proxy:v1.28.1
[config/images] Pulled registry.k8s.io/pause:3.9
[config/images] Pulled registry.k8s.io/etcd:3.5.9-0
[config/images] Pulled registry.k8s.io/coredns/coredns:v1.10.1
#+end_example

** Inspect kubernetes container images
#+begin_src bash :sync
sudo crictl images
#+end_src

#+RESULTS:
#+begin_example
IMAGE                                     TAG                 IMAGE ID            SIZE
registry.k8s.io/coredns/coredns           v1.10.1             ead0a4a53df89       16.2MB
registry.k8s.io/etcd                      3.5.9-0             73deb9a3f7025       103MB
registry.k8s.io/kube-apiserver            v1.28.1             5c801295c21d0       34.6MB
registry.k8s.io/kube-controller-manager   v1.28.1             821b3dfea27be       33.4MB
registry.k8s.io/kube-proxy                v1.28.1             6cdbabde3874e       24.6MB
registry.k8s.io/kube-scheduler            v1.28.1             b462ce0c8b1ff       18.8MB
registry.k8s.io/pause                     3.9                 e6f1816883972       322kB
#+end_example

** Initialize our cluster
#+begin_src tmux
kubeadm init --config /etc/kubernetes/kubeadm-config.yaml
#+end_src
** Configure our KUBECONFIG
#+begin_src tmux
mkdir -p $HOME/.kube
sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
#+end_src
** wait for apiserver and untaint control plane
I don't think we need this anymore
#+begin_src tmux
until kubectl get --raw='/readyz?verbose'; do sleep 5; done
echo kubectl taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule-
#+end_src
** Gateway API
- https://isovalent.com/blog/post/tutorial-getting-started-with-the-cilium-gateway-api/
Looks like there's a new version at:
- https://gateway-api.sigs.k8s.io/guides/#install-standard-channel

#+begin_src tmux
kubectl apply -f  \
    https://github.com/kubernetes-sigs/gateway-api/releases/download/v0.7.1/experimental-install.yaml
#+end_src

#+RESULTS:
#+begin_example
customresourcedefinition.apiextensions.k8s.io/gatewayclasses.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/gateways.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/httproutes.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/referencegrants.gateway.networking.k8s.io created
namespace/gateway-system created
validatingwebhookconfiguration.admissionregistration.k8s.io/gateway-api-admission created
service/gateway-api-admission-server created
deployment.apps/gateway-api-admission-server created
serviceaccount/gateway-api-admission created
clusterrole.rbac.authorization.k8s.io/gateway-api-admission created
clusterrolebinding.rbac.authorization.k8s.io/gateway-api-admission created
role.rbac.authorization.k8s.io/gateway-api-admission created
rolebinding.rbac.authorization.k8s.io/gateway-api-admission created
job.batch/gateway-api-admission created
job.batch/gateway-api-admission-patch created
#+end_example

** cni: cilium
You may need to run this a few times, and make sure gateway-system is up before you start.
#+begin_src tmux
helm repo add cilium https://helm.cilium.io/
helm upgrade --install cilium cilium/cilium \
    --version 1.14.1 \
    --namespace kube-system \
    -f /etc/kubernetes/cilium-values.yaml
#+end_src
** wait for our node to be Ready
Cluster should be up at this point
#+begin_src tmux
kubectl wait --for=condition=Ready \
    --selector=node-role.kubernetes.io/control-plane="" \
    --timeout=120s node
#+end_src
** copy our kubeconfig local
#+begin_src shell :sync :results silent
scp root@$SPACE_TLD:/etc/kubernetes/admin.conf $KUBECONFIG
#+end_src

** increase maxPods
Our nodes usually run a lot of pods, so the default of 110 is way to low, so we bump it by a magnitude of roughly ten.

https://prefetch.net/blog/2018/02/10/the-kubernetes-110-pod-limit-per-node/

It needs to be set in the kublet config file:
https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/

Ideally we set this via kubeadm, but for now manually add it and restart kubelet.
#+begin_src shell :eval never
echo maxPods: 1024 >> /var/lib/kubelet/config.yaml
#+end_src

#+begin_src bash :sync
grep maxPods /var/lib/kubelet/config.yaml
#+end_src

#+RESULTS:
#+begin_example
maxPods: 1024
#+end_example

#+begin_src bash :eval never
systemctl restart kubelet
#+end_src

#+begin_src bash :sync
kubectl describe nodes  | grep Capacity: -A6
#+end_src

#+RESULTS:
#+begin_example
Capacity:
  cpu:                64
  ephemeral-storage:  227158056Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             263515036Ki
  pods:               1024
#+end_example

** increase fswatch

*** temporary fix
#+begin_src bash
sysctl fs.inotify.max_user_watches=524288
sysctl fs.inotify.max_user_instances=512
#+end_src

#+RESULTS:
#+begin_example
fs.inotify.max_user_watches = 524288
fs.inotify.max_user_instances = 512
#+end_example

*** persistent fix
To make the changes persistent, edit the file /etc/sysctl.d/99-maxnotify.conf and add these lines:
#+begin_src toml :tangle (concat tramp-dir "etc/sysctl.d/99-maxnotify.conf") :comments no
fs.inotify.max_user_watches = 524288
fs.inotify.max_user_instances = 512
#+end_src
You will then need load settings from that file:
#+begin_src shell
sysctl --system
#+end_src

#+RESULTS:
#+begin_example
#+end_example

*** verify
#+begin_src bash :sync
sysctl get fs.inotify.max_user_watches
sysctl get fs.inotify.max_user_instances
#+end_src

#+RESULTS:
#+begin_example
fs.inotify.max_user_watches = 524288
fs.inotify.max_user_instances = 512
#+end_example

* Bootstrap Fux + Sops Encryption
** generate a github TOKEN
https://github.com/settings/tokens/new
https://github.com/settings/personal-access-tokens/new
Make sure it's for the right organization
- Administration :: Access: Read and write
- Contents :: Access: Read and write
- Metadata :: Access: Read-only

** setup gh cli and authenticate
#+begin_src tmux
sudo apt-get install gh
#+end_src
** GITHUB_TOKEN
#+begin_src shell :sync
echo export GITHUB_TOKEN=$GITHUB_TOKEN
#+end_src

** Make sure the token has admin access if it needs to add the repo
Otherwise ensure the repo and branch exist, and the token has ability to configure hook s
** bootstrap flux
This needs to be done to the correct folder, owner, and repo...
#+begin_src tmux
flux bootstrap github --branch=container --owner=sharingio --repository=infra --path=clusters/container.sharing.io
#+end_src
#+begin_example
? What account do you want to log into? GitHub.com
? What is your preferred protocol for Git operations? HTTPS
? Authenticate Git with your GitHub credentials? Yes
? How would you like to authenticate GitHub CLI?  [Use arrows to move, type to filter]
  Login with a web browser
> Paste an authentication token
#+end_example
** TODO at this point check out the repo and put this file into ./clusters/thinkpad/ or similar
#+begin_src tmux
git clone git@github.com:sharingio/infra || gh repo clone sharingio/infra
cp this.org infra/clusters/NEW/setup.org
#+end_src
** Setup SOPS + Flux
*** install sops binary
**** linux
#+begin_src tmux
wget https://github.com/getsops/sops/releases/download/v3.7.3/sops_3.7.3_amd64.deb
sudo dpkg -i sops_*deb
rm sops_*deb
#+end_src
**** mac
#+begin_src bash
brew install gnupg sops
#+end_src
*** generate gpg key
#+begin_src tmux :env KEY_NAME="k8s.container.sharing.io" :sync :results silent
export KEY_NAME="k8s.container.sharing.io"
export KEY_COMMENT="flux secrets"

gpg --batch --full-generate-key <<EOF
%no-protection
Key-Type: 1
Key-Length: 4096
Subkey-Type: 1
Subkey-Length: 4096
Expire-Date: 0
Name-Comment: ${KEY_COMMENT}
Name-Real: ${KEY_NAME}
EOF
#+end_src

*** list gpg keys
#+begin_src tmux :env KEY_NAME="k8s.container.sharing.io" :sync
export KEY_NAME="k8s.container.sharing.io"
gpg --list-secret-keys "${KEY_NAME}"
#+end_src

#+RESULTS:
#+begin_example
sec   rsa4096 2023-08-28 [SCEAR]
      6CFB156EEFE7417E2567D5FD37732CFA004DC229
uid           [ultimate] k8s.uk.sharing.io (flux secrets)
ssb   rsa4096 2023-08-28 [SEAR]

#+end_example

*** import into kubernetes
#+begin_src tmux :env KEY_NAME="k8s.container.sharing.io" :sync
export KEY_NAME="k8s.container.sharing.io"
export KEY_FP=$(gpg --list-secret-keys "${KEY_NAME}" | grep SCEA -A1 | tail -1 | awk '{print $1}')
kubectl delete secret sops-gpg --namespace=flux-system || true
gpg --export-secret-keys --armor "${KEY_FP}" |
kubectl create secret generic sops-gpg \
--namespace=flux-system \
--from-file=sops.asc=/dev/stdin
#+end_src

#+RESULTS:
#+begin_example
#+end_example

*** export key into git
#+begin_src tmux :env KEY_NAME="k8s.container.sharing.io" :results silent :sync
export KEY_NAME="k8s.container.sharing.io"
export KEY_FP=$(gpg --list-secret-keys "${KEY_NAME}" | grep SCEA -A1 | tail -1 | awk '{print $1}')
# gpg --export --armor "${KEY_FP}" > ./clusters/thinkpad/.sops.pub.asc
cd ~/sharingio/infra/clusters/container.sharing.io
gpg --export --armor "${KEY_FP}" > .sops.pub.asc
#+end_src

*** write SOPS config file
#+begin_src tmux
export KEY_NAME="k8s.container.sharing.io"
export KEY_FP=$(gpg --list-secret-keys "${KEY_NAME}" | grep SCEA -A1 | tail -1 | awk '{print $1}')
cd ~/sharingio/infra/clusters/container.sharing.io
cat <<EOF >> ./.sops.yaml
creation_rules:
  - path_regex: .*.yaml
    encrypted_regex: ^(data|stringData)$
    pgp: ${KEY_FP}
EOF
#+end_src
* PDNS setup self hosted zones from scratch
** via pdnsutil within auth-dns
*** create a zone with NS
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -ti -- \
  pdnsutil create-zone sharing.io ns.ii.coop
#+end_src

#+RESULTS:
#+begin_example
Also adding one NS record
#+end_example
*** records
**** add second NS to zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -ti -- \
  pdnsutil add-record sharing.io @ NS ns.workshop.coop.
#+end_src

#+RESULTS:
#+begin_example
New rrset:
sharing.io. 3600 IN NS ns.ii.coop
sharing.io. 3600 IN NS ns.workshop.coop
#+end_example
**** add A for zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
  pdnsutil add-record sharing.io @ A 136.144.49.79
#+end_src

#+RESULTS:
#+begin_example
New rrset:
sharing.io. 3600 IN A 136.144.49.79
#+end_example
**** add wildcard A for zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
  pdnsutil add-record sharing.io '*' A 136.144.49.79
#+end_src

#+RESULTS:
#+begin_example
New rrset:
,*.sharing.io. 3600 IN A 136.144.49.79
#+end_example
**** list zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil list-zone sharing.io
#+end_src

#+RESULTS:
#+begin_example
$ORIGIN .
,*.sharing.io	3600	IN	A	136.144.49.79
sharing.io	3600	IN	A	136.144.49.79
sharing.io	3600	IN	NS	ns.ii.coop.
sharing.io	3600	IN	NS	ns.workshop.coop.
sharing.io	3600	IN	SOA	ns.sharing.io hostmaster.sharing.io 0 10800 3600 604800 3600
#+end_example
*** tsig key + activation
**** import-tsig-key (from vars)
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil import-tsig-key $PDNS_TSIG_KEYNAME $PDNS_TSIG_ALGO $PDNS_TSIG_KEY
#+end_src

#+RESULTS:
#+begin_example
Imported TSIG key sharing.io hmac-sha256
#+end_example
**** list-tsig-keys
#+begin_src shell :sync :results silent
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil list-tsig-keys
#+end_src
**** activate-tsig-key
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil activate-tsig-key $DNS_ZONE $PDNS_TSIG_KEYNAME primary
#+end_src

#+RESULTS:
#+begin_example
Enabled TSIG key sharing.io for sharing.io
#+end_example
**** activate-tsig-key
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
 pdnsutil set-meta $DNS_ZONE TSIG-ALLOW-DNSUPDATE $PDNS_TSIG_KEYNAME
#+end_src

#+RESULTS:
#+begin_example
Set 'sharing.io' meta TSIG-ALLOW-DNSUPDATE = sharing.io
#+end_example

**** show zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil show-zone sharing.io
#+end_src

#+RESULTS:
#+begin_example
This is a Native zone
Zone is not actively secured
Zone has following allowed TSIG key(s): sharing.io
Metadata items:
	TSIG-ALLOW-AXFR	sharing.io
	TSIG-ALLOW-DNSUPDATE	sharing.io
#+end_example
**** get meta
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil get-meta sharing.io
#+end_src

#+RESULTS:
#+begin_example
Metadata for 'sharing.io'
TSIG-ALLOW-AXFR = sharing.io
TSIG-ALLOW-DNSUPDATE = sharing.io
#+end_example

**** check all zones
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil check-all-zones
#+end_src

#+RESULTS:
#+begin_example
Checked 5 records of 'sharing.io', 0 errors, 0 warnings.
Checked 1 zones, 0 had errors.
#+end_example

*** nsupdate using TSIG_KEY
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :wrap "example" :sync
nsupdate -y "$PDNS_TSIG_ALGO:$PDNS_TSIG_KEYNAME:$PDNS_TSIG_KEY" <<EOF
update add localhost.$DNS_ZONE 60 A 127.0.0.1
send
EOF
#+end_src

#+RESULTS:
#+begin_example
#+end_example

*** show zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil show-zone sharing.io
#+end_src

#+RESULTS:
#+begin_example
This is a Native zone
Zone is not actively secured
Zone has following allowed TSIG key(s): sharing.io
Metadata items:
	TSIG-ALLOW-AXFR	sharing.io
	TSIG-ALLOW-DNSUPDATE	sharing.io
#+end_example

** subzone via pdnsutil
*** create NS records for new zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -ti -- \
  pdnsutil add-record container.sharing.io @ NS ns.workshop.coop.
kubectl -n powerdns exec svc/auth-dns -ti -- \
  pdnsutil add-record container.sharing.io @ NS ns.ii.coop.
#+end_src

#+RESULTS:
#+begin_example
New rrset:
container.sharing.io. 3600 IN NS ns.ii.coop
container.sharing.io. 3600 IN NS ns.workshop.coop
container.sharing.io. 3600 IN NS ns.workshop.coop
#+end_example

*** create a zone with NS
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -ti -- \
  pdnsutil create-zone container.sharing.io ns.ii.coop
#+end_src

#+RESULTS:
#+begin_example
Also adding one NS record
#+end_example
*** records
**** add second NS to zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -ti -- \
  pdnsutil add-record container.sharing.io @ NS ns.workshop.coop.
#+end_src

#+RESULTS:
#+begin_example
New rrset:
container.sharing.io. 3600 IN NS ns.ii.coop
container.sharing.io. 3600 IN NS ns.workshop.coop
#+end_example

**** add A for zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
  pdnsutil add-record container.sharing.io @ A 136.144.49.79
#+end_src

#+RESULTS:
#+begin_example
New rrset:
container.sharing.io. 3600 IN A 136.144.49.79
#+end_example
**** add wildcard A for zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
  pdnsutil add-record container.sharing.io '*' A 136.144.49.79
#+end_src

#+RESULTS:
#+begin_example
New rrset:
,*.container.sharing.io. 3600 IN A 136.144.49.79
#+end_example
**** list zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil list-zone container.sharing.io
#+end_src

#+RESULTS:
#+begin_example
$ORIGIN .
,*.container.sharing.io	3600	IN	A	136.144.49.79
container.sharing.io	3600	IN	A	136.144.49.79
container.sharing.io	3600	IN	NS	ns.ii.coop.
container.sharing.io	3600	IN	NS	ns.workshop.coop.
container.sharing.io	3600	IN	SOA	ns.sharing.io hostmaster.sharing.io 0 10800 3600 604800 3600
#+end_example
**** activate-tsig-key
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil activate-tsig-key container.$DNS_ZONE $PDNS_TSIG_KEYNAME primary
#+end_src

#+RESULTS:
#+begin_example
Enabled TSIG key sharing.io for container.sharing.io
#+end_example
**** tsig-allow-dnsupdate
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
 pdnsutil set-meta container.$DNS_ZONE TSIG-ALLOW-DNSUPDATE $PDNS_TSIG_KEYNAME
#+end_src

#+RESULTS:
#+begin_example
Set 'container.sharing.io' meta TSIG-ALLOW-DNSUPDATE = sharing.io
#+end_example
**** get meta
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil get-meta container.sharing.io
#+end_src

#+RESULTS:
#+begin_example
Metadata for 'container.sharing.io'
TSIG-ALLOW-AXFR = sharing.io
TSIG-ALLOW-DNSUPDATE = sharing.io
#+end_example


*** tsig key + activation
**** import-tsig-key (from vars)
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil import-tsig-key $PDNS_TSIG_KEYNAME $PDNS_TSIG_ALGO $PDNS_TSIG_KEY
#+end_src

#+RESULTS:
#+begin_example
Imported TSIG key sharing.io hmac-sha256
#+end_example
**** list-tsig-keys
#+begin_src shell :sync :results silent
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil list-tsig-keys
#+end_src
**** activate-tsig-key
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil activate-tsig-key $DNS_ZONE $PDNS_TSIG_KEYNAME primary
#+end_src

#+RESULTS:
#+begin_example
Enabled TSIG key sharing.io for sharing.io
#+end_example
**** tsig-allow-dnsupdate
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
 pdnsutil set-meta $DNS_ZONE TSIG-ALLOW-DNSUPDATE $PDNS_TSIG_KEYNAME
#+end_src

#+RESULTS:
#+begin_example
Set 'sharing.io' meta TSIG-ALLOW-DNSUPDATE = sharing.io
#+end_example

**** show zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil show-zone sharing.io
#+end_src

#+RESULTS:
#+begin_example
This is a Native zone
Zone is not actively secured
Zone has following allowed TSIG key(s): sharing.io
Metadata items:
	TSIG-ALLOW-AXFR	sharing.io
	TSIG-ALLOW-DNSUPDATE	sharing.io
#+end_example
**** get meta
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil get-meta sharing.io
#+end_src

#+RESULTS:
#+begin_example
Metadata for 'sharing.io'
TSIG-ALLOW-AXFR = sharing.io
TSIG-ALLOW-DNSUPDATE = sharing.io
#+end_example

**** check all zones
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil check-all-zones
#+end_src

#+RESULTS:
#+begin_example
Checked 5 records of 'sharing.io', 0 errors, 0 warnings.
Checked 1 zones, 0 had errors.
#+end_example

*** nsupdate using TSIG_KEY
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :wrap "example" :sync
nsupdate -y "$PDNS_TSIG_ALGO:$PDNS_TSIG_KEYNAME:$PDNS_TSIG_KEY" <<EOF
update add localhost.$DNS_ZONE 60 A 127.0.0.1
send
EOF
#+end_src

#+RESULTS:
#+begin_example
#+end_example

*** show zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil show-zone sharing.io
#+end_src

#+RESULTS:
#+begin_example
This is a Native zone
Zone is not actively secured
Zone has following allowed TSIG key(s): sharing.io
Metadata items:
	TSIG-ALLOW-AXFR	sharing.io
	TSIG-ALLOW-DNSUPDATE	sharing.io
#+end_example

** subzone via pdnsutil
*** create NS records for new zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -ti -- \
  pdnsutil add-record uk.sharing.io @ NS ns.workshop.coop.
kubectl -n powerdns exec svc/auth-dns -ti -- \
  pdnsutil add-record uk.sharing.io @ NS ns.ii.coop.
#+end_src

#+RESULTS:
#+begin_example
New rrset:
uk.sharing.io. 3600 IN NS ns.workshop.coop
uk.sharing.io. 3600 IN NS ns.ii.coop
uk.sharing.io. 3600 IN NS ns.workshop.coop
#+end_example

*** create a zone with NS
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -ti -- \
  pdnsutil create-zone uk.sharing.io ns.ii.coop
#+end_src

#+RESULTS:
#+begin_example
Also adding one NS record
#+end_example
*** records
**** add second NS to zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -ti -- \
  pdnsutil add-record uk.sharing.io @ NS ns.workshop.coop.
#+end_src

#+RESULTS:
#+begin_example
New rrset:
uk.sharing.io. 3600 IN NS ns.workshop.coop
#+end_example

**** add second NS to zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -ti -- \
  pdnsutil add-record uk.sharing.io @ NS ns.ii.coop.
#+end_src

#+RESULTS:
#+begin_example
New rrset:
uk.sharing.io. 3600 IN NS ns.workshop.coop
uk.sharing.io. 3600 IN NS ns.ii.coop
#+end_example

**** add A for zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
  pdnsutil add-record uk.sharing.io @ A 145.40.113.253
#+end_src

#+RESULTS:
#+begin_example
New rrset:
uk.sharing.io. 3600 IN A 145.40.113.253
#+end_example
**** add wildcard A for zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
  pdnsutil add-record uk.sharing.io '*' A 145.40.113.253
#+end_src

#+RESULTS:
#+begin_example
New rrset:
,*.uk.sharing.io. 3600 IN A 145.40.113.253
#+end_example
**** list zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil list-zone uk.sharing.io
#+end_src

#+RESULTS:
#+begin_example
$ORIGIN .
,*.uk.sharing.io	3600	IN	A	145.40.113.253
uk.sharing.io	3600	IN	A	145.40.113.253
uk.sharing.io	3600	IN	NS	ns.workshop.coop.
uk.sharing.io	3600	IN	NS	ns.ii.coop.
uk.sharing.io	3600	IN	SOA	ns.sharing.io hostmaster.sharing.io 0 10800 3600 604800 3600
#+end_example
**** activate-tsig-key
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil activate-tsig-key uk.$DNS_ZONE $PDNS_TSIG_KEYNAME primary
#+end_src

#+RESULTS:
#+begin_example
Enabled TSIG key sharing.io for uk.sharing.io
#+end_example
**** tsig-allow-dnsupdate
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
 pdnsutil set-meta uk.$DNS_ZONE TSIG-ALLOW-DNSUPDATE $PDNS_TSIG_KEYNAME
#+end_src

#+RESULTS:
#+begin_example
Set 'uk.sharing.io' meta TSIG-ALLOW-DNSUPDATE = sharing.io
#+end_example
**** get meta
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil get-meta uk.sharing.io
#+end_src

#+RESULTS:
#+begin_example
Metadata for 'uk.sharing.io'
TSIG-ALLOW-AXFR = sharing.io
TSIG-ALLOW-DNSUPDATE = sharing.io
#+end_example

*** tsig key + activation
**** import-tsig-key (from vars)
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil import-tsig-key $PDNS_TSIG_KEYNAME $PDNS_TSIG_ALGO $PDNS_TSIG_KEY
#+end_src

#+RESULTS:
#+begin_example
Imported TSIG key sharing.io hmac-sha256
#+end_example
**** list-tsig-keys
#+begin_src shell :sync :results silent
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil list-tsig-keys
#+end_src
**** activate-tsig-key
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil activate-tsig-key $DNS_ZONE $PDNS_TSIG_KEYNAME primary
#+end_src

#+RESULTS:
#+begin_example
Enabled TSIG key sharing.io for sharing.io
#+end_example
**** tsig-allow-dnsupdate
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
 pdnsutil set-meta $DNS_ZONE TSIG-ALLOW-DNSUPDATE $PDNS_TSIG_KEYNAME
#+end_src

#+RESULTS:
#+begin_example
Set 'sharing.io' meta TSIG-ALLOW-DNSUPDATE = sharing.io
#+end_example

**** show zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil show-zone sharing.io
#+end_src

#+RESULTS:
#+begin_example
This is a Native zone
Zone is not actively secured
Zone has following allowed TSIG key(s): sharing.io
Metadata items:
	TSIG-ALLOW-AXFR	sharing.io
	TSIG-ALLOW-DNSUPDATE	sharing.io
#+end_example
**** get meta
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil get-meta sharing.io
#+end_src

#+RESULTS:
#+begin_example
Metadata for 'sharing.io'
TSIG-ALLOW-AXFR = sharing.io
TSIG-ALLOW-DNSUPDATE = sharing.io
#+end_example

**** check all zones
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil check-all-zones
#+end_src

#+RESULTS:
#+begin_example
Checked 5 records of 'sharing.io', 0 errors, 0 warnings.
Checked 1 zones, 0 had errors.
#+end_example

*** nsupdate using TSIG_KEY
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :wrap "example" :sync
nsupdate -y "$PDNS_TSIG_ALGO:$PDNS_TSIG_KEYNAME:$PDNS_TSIG_KEY" <<EOF
update add localhost.$DNS_ZONE 60 A 127.0.0.1
send
EOF
#+end_src

#+RESULTS:
#+begin_example
#+end_example

*** show zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil show-zone sharing.io
#+end_src

#+RESULTS:
#+begin_example
This is a Native zone
Zone is not actively secured
Zone has following allowed TSIG key(s): sharing.io
Metadata items:
	TSIG-ALLOW-AXFR	sharing.io
	TSIG-ALLOW-DNSUPDATE	sharing.io
#+end_example

** via pdns api
*** get powerdns server config
We can't reach powerdns yet so we forward
#+begin_src tmux
kubectl -n powerdns port-forward svc/auth-web 8081:8081
#+end_src
*** get powerdns server config
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :wrap "src json"
curl -L -s \
    -H "X-API-Key: $PDNS_API_KEY" \
    -H "Content-Type: text/plain" \
    http://localhost:8081/api/v1/servers/localhost/config | jq .
#+end_src

*** get powerdns server config
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :wrap "src json"
curl -L -s \
    -H "X-API-Key: $PDNS_API_KEY" \
    -H "Content-Type: text/plain" \
    http://localhost:8081/api/v1/servers/localhost/config | jq .
#+end_src
*** create tsig key
https://doc.powerdns.com/authoritative/http-routingtable.html
https://doc.powerdns.com/authoritative/http-api/tsigkey.html#post--servers-server_id-tsigkeys
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :sync
curl -L -s \
    -H "X-API-Key: $PDNS_API_KEY" \
    -H "Content-Type: application/json" \
    -d "{\"name\": \"$PDNS_TSIG_KEYNAME\", \"algorithm\": \"hmac-sha256\" }" \
    http://localhost:8081/api/v1/servers/localhost/tsigkeys | jq .
#+end_src
*** list tsig keys

#+begin_src shell :sync
curl -L -vvv \
    -H "X-API-Key: $PDNS_API_KEY" \
    -H "Content-Type: application/json" \
    http://localhost:8081/api/v1/servers/localhost/tsigkeys | jq .
#+end_src


*** set TSIG-ALLOW-DNSUPDATE
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :sync
curl -L -s \
    -H "X-API-Key: $PDNS_API_KEY" \
    -H "Content-Type: application/json" \
    -d "{\"kind\": \"TSIG-ALLOW-DNSUPDATE\", \"metadata\": [\"${PDNS_TSIG_KEYNAME}.\"]}" \
    http://localhost:8081/api/v1/servers/localhost/$DNS_ZONE/metadata \
    # | jq .
#+end_src

#+RESULTS:
#+begin_example
Not Found
#+end_example
* cert manager
** list zone
#+begin_src shell :sync
kubectl -n powerdns exec svc/auth-dns -- \
    pdnsutil list-zone sharing.io
#+end_src

#+RESULTS:
#+begin_example
$ORIGIN .
localhost.sharing.io	60	IN	A	127.0.0.1
,*.sharing.io	3600	IN	A	136.144.49.79
sharing.io	3600	IN	A	136.144.49.79
sharing.io	3600	IN	NS	ns.ii.coop.
sharing.io	3600	IN	NS	ns.workshop.coop.
sharing.io	3600	IN	SOA	ns.sharing.io hostmaster.sharing.io 2023091101 10800 3600 604800 3600
#+end_example
* PDNS api Secret
https://github.com/zachomedia/cert-manager-webhook-pdns#powerdns-cert-manager-acme-webhook
Create one here with only access to uk.sharing.io
https://powerdns.ii.nz/admin/manage-keys
** cert-manager
*** create pdns secret
Note that the TSIG_KEY we retrieve is base64 encoded... it get's double encoded as a kubernetes secret. Most places you use a TSIG_KEY are expecting the base64 value we have here.
#+begin_src tmux :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :sync :results silent
cd ~/sharingio/infra/clusters/container.sharing.io
source .envrc
mkdir -p secrets
kubectl -n cert-manager \
    create secret generic pdns \
    --from-literal=api-key="$PDNS_API_KEY" \
    --dry-run=client -o yaml > \
    ./secrets/cert-manager-pdns.yaml
#+end_src

*** encrypt pdns secret
We need to encrypt the pdns-secret with sops and commit/push so flux can decrypt and apply it
#+begin_src tmux :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :results silent :sync
sops --encrypt --in-place ./secrets/cert-manager-pdns.yaml
#+end_src

* minio
** Login url + user / password
https://minio.uk.sharing.io/login
Username :  $MINIO_ROOT_USER
Password : $MINI_ROOT_PASSWORD
** mc alias
#+begin_src shell
mc alias set min https://s3.uk.sharing.io $MINIO_ROOT_USER $MINIO_ROOT_PASSWORD
#+end_src

#+RESULTS:
#+begin_example
Added `min` successfully.
#+end_example
** mc mb min
#+begin_src shell
mc mb min/sharingio
#+end_src

#+RESULTS:
#+begin_example
Bucket created successfully `uk/sharingio`.
#+end_example

** mc ls min
#+begin_src shell
mc ls min
#+end_src

#+RESULTS:
#+begin_example
[2023-08-23 10:56:26 BST]     0B sharingio/
#+end_example

** minio-env-config
This will eventually be a file within the minio-pool:
#+begin_src shell
 kubectl -n minio exec pod/minio-pool-0-0 cat /tmp/minio/config.env
#+end_src

*** create minio secret
This is basically a file mapping for an env file called config.env

MINIO_ROOT_USER / MINIO_ACCESS_KEY must be at least 3 characters long... we'll default to "root"

NOTE: Access key length should be at least 3, and secret key length at least 8 characters

#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :sync :results silent
source .envrc
kubectl -n minio \
    create secret generic minio-env-config \
    --from-literal=config.env="export MINIO_ROOT_USER=$MINIO_ROOT_USER
export MINIO_ROOT_PASSWORD=$MINIO_ROOT_PASSWORD
export MINIO_IDENTITY_OPENID_CLIENT_ID=$MINIO_IDENTITY_OPENID_CLIENT_ID
export MINIO_IDENTITY_OPENID_CLIENT_SECRET=$MINIO_IDENTITY_OPENID_CLIENT_SECRET" \
    --dry-run=client -o yaml > \
     ./secrets/minio-env-config.yaml
#+end_src

# export MINIO_ACCESS_KEY=$MINIO_ROOT_USER
# export MINIO_SECRET_KEY=$MINIO_ROOT_PASSWORD" \
*** encrypt minio secret
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :results silent :sync
sops --encrypt --in-place ./secrets/minio-env-config.yaml
#+end_src
*** the secret
It's a single key, but it contains the text of the env.
#+begin_src shell :sync
 kubectl -n minio describe secret minio-env-config
#+end_src

#+RESULTS:
#+begin_example
Name:         minio-env-config
Namespace:    minio
Labels:       kustomize.toolkit.fluxcd.io/name=secrets
              kustomize.toolkit.fluxcd.io/namespace=flux-system
Annotations:  <none>

Type:  Opaque

Data
====
config.env:  319 bytes
#+end_example

** longhorn-minio
*** create minio secret
This is basically a file mapping for an env file called config.env

MINIO_ROOT_USER / MINIO_ACCESS_KEY must be at least 3 characters long... we'll default to "root"

NOTE: Access key length should be at least 3, and secret key length at least 8 characters

#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :sync :results silent
source .envrc
kubectl -n longhorn \
    create secret generic longhorn-minio \
    --from-literal=AWS_ACCESS_KEY_ID="$MINIO_ROOT_USER" \
    --from-literal=AWS_SECRET_ACCESS_KEY="$MINIO_ROOT_PASSWORD" \
    --from-literal=AWS_ENDPOINTS="https://s3.uk.sharing.io" \
    --dry-run=client -o yaml > \
     ./secrets/longhorn-minio.yaml
#+end_src
*** encrypt minio secret
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :results silent :sync
sops --encrypt --in-place ./secrets/longhorn-minio.yaml
#+end_src
*** the secret
Longhorn wants a minio secret to backup to
#+begin_src shell :sync
 kubectl -n longhorn describe secret longhorn-minio
#+end_src

#+RESULTS:
#+begin_example
#+end_example

** OIDC
https://goauthentik.io/integrations/services/minio/
#+begin_src shell
mc mb uk/abcs
#+end_src

#+RESULTS:
#+begin_example
Bucket created successfully `uk/abcs`.
#+end_example

https://min.io/docs/minio/linux/operations/external-iam/configure-openid-external-identity-management.html
* PowerDNS
[[https://pdns.sharing.io]]
Username admin
Password $PDNS_API_KEY

** create powerdns secret
This is basically a file mapping for an env file called config.env
#+begin_src tmux
source .envrc
kubectl -n powerdns \
    create secret generic powerdns \
    --from-literal=api_key=${PDNS_API_KEY} \
    --from-literal=admin_user=${PDNS_ADMIN_USER} \
    --from-literal=admin_email=${PDNS_ADMIN_EMAIL} \
    --from-literal=admin_password=${PDNS_ADMIN_PASSWORD} \
    --from-literal=sql_url=postgresql://postgres:${PDNS_DB_PASSWORD}@pdns-db-postgresql:5432/postgres \
    --from-literal=postgres_password=${PDNS_DB_PASSWORD} \
    --dry-run=client -o yaml > \
     ./secrets/powerdns.yaml
#+end_src

** encrypt powerdns secret
#+begin_src tmux
sops --encrypt --in-place ./secrets/powerdns.yaml
#+end_src

* gitops
** update the gitops.yaml with

Ideally we could store this in a secret... but the helm chart doesn't seem to easily allow that.
flux folks are wanting you to use this to bootstrap and move towards SSO.

#+begin_src shell :sync
echo "$GITOPS_PASSWORD" | gitops get bcrypt-hash
#+end_src

** dashboard
*** ensure GITOPS_PASSWORD is set
#+begin_src bash :sync :dir . :results silent
. .envrc
gitops create dashboard ww-gitops \
  --password=$GITOPS_PASSWORD \
  --export > .../gitops-dashboard.yaml
#+end_src

** reciever
*** setup webhook
https://github.com/cloudnative-nz/infra/settings/hooks/new
**** generate HMAC
#+name: new_hmac
#+begin_src shell :sync
TOKEN=$(head -c 12 /dev/urandom | shasum | cut -d ' ' -f1)
echo export FLUX_RECEIVER_TOKEN=$TOKEN >> .envrc
#+end_src

**** check env
#+begin_src bash :sync :dir . :results silent
. .envrc
echo -n $FLUX_RECEIVER_TOKEN
#+end_src

**** create receiven-token secrets
#+begin_src tmux
. .envrc
kubectl -n flux-system create secret \
    --dry-run=client -o yaml \
    generic receiver-token \
    --from-literal=token=$FLUX_RECEIVER_TOKEN > ./secrets/flux-receiver.yaml
#+end_src
*** encrypt and commit TSIG secret
We need to encrypt the secret with sops and commit/push so flux can decrypt and apply it
#+begin_src tmux
sops --encrypt --in-place ./secrets/flux-receiver.yaml
#+end_src

*** get the ingress
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :dir . :sync
kubectl -n flux-system get ingress webhook-receiver
#+end_src

#+RESULTS:
#+begin_example
NAME               CLASS   HOSTS                               ADDRESS         PORTS     AGE
webhook-receiver   nginx   flux-webhook.container.sharing.io   136.144.49.79   80, 443   3m35s
#+end_example

*** get the hook path
We need to encrypt the secret with sops and commit/push so flux can decrypt and apply it
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :dir . :sync
kubectl -n flux-system get receiver
#+end_src

#+RESULTS:
#+begin_example
NAME              AGE     READY   STATUS
github-receiver   6m43s   True    Receiver initialized for path: /hook/e40123b877be41ea5bf7c2712ebf1fbaec7a0176e1342f60c1848bf8b25b1fbb
#+end_example
*** combine them into something like
Use the Secret and this PayloadURL to create a new webook:
[[https://github.com/sharingio/infra/settings/hooks]]
[[https://github.com/sharingio/infra/settings/hooks/new]]
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :dir . :sync
echo PayloadURL: https://$(kubectl -n flux-system get ingress webhook-receiver -o jsonpath="{.spec.rules[0].host}")$(kubectl -n flux-system get receiver github-receiver -o jsonpath="{.status.webhookPath}")
#+end_src

#+RESULTS:
#+begin_example
PayloadURL: https://flux-webhook.container.sharing.io/hook/e40123b877be41ea5bf7c2712ebf1fbaec7a0176e1342f60c1848bf8b25b1fbb
#+end_example
** oidc-access
From https://docs.gitops.weave.works/docs/configuration/oidc-access/#configuration
- https://docs.gitops.weave.works/docs/configuration/oidc-access/#scopes
*** create flux-system oidc-auth secret
#+begin_src tmux
. .envrc
kubectl -n flux-system create secret \
    --dry-run=client -o yaml \
    generic oidc-auth \
    --from-literal=issuerURL=https://sso.sharing.io/application/o/container-gitops/ \
    --from-literal=clientID=$GITOPS_OIDC_CLIENT_ID \
    --from-literal=clientSecret=$GITOPS_OIDC_CLIENT_SECRET \
    --from-literal=redirectURL=https://gitops.container.sharing.io/oauth2/callback \
    --from-literal=tokenDuration=1h0m0s \
    --from-literal=claimUsername=email \
    --from-literal=claimGroups=groups \
    > ./secrets/gitops-oidc.yaml
#+end_src
*** encrypt flux-system oidc-auth secret
We need to encrypt the secret with sops and commit/push so flux can decrypt and apply it
#+begin_src tmux
sops --encrypt --in-place ./secrets/gitops-oidc.yaml
#+end_src

* authentic-env
[[https://sso.sharing.io/]]
Username "akadmin"
Password $AUTHENTIK_BOOTSTRAP_PASSWORD
** secret
*** create authentik-env secret
This is basically a file mapping for an env file called config.env
#+begin_src tmux
source .envrc
kubectl -n authentik \
    create secret generic authentik-env \
    --from-literal=AUTHENTIK_BOOTSTRAP_PASSWORD=$AUTHENTIK_BOOTSTRAP_PASSWORD \
    --from-literal=AUTHENTIK_BOOTSTRAP_TOKEN=$AUTHENTIK_BOOTSTRAP_TOKEN \
    --from-literal=AUTHENTIK_SECRET_KEY=$AUTHENTIK_SECRET_KEY \
    --dry-run=client -o yaml > \
     ./secrets/authentik-env.yaml
#+end_src

*** encrypt authentik-env secret
#+begin_src tmux
sops --encrypt --in-place ./secrets/authentik-env.yaml
#+end_src
** powerdns-admin
*** get authentik Self-signed certificate
https://sso.uk.sharing.io/if/admin/#/crypto/certificates

* Ghost
** ghost-secret
https://github.com/bitnami/charts/blob/main/bitnami/mysql/values.yaml#L138-L141
#+begin_src shell :prologue "(\nexport KUBECONFIG" :epilogue "\n) 2>&1\n\:\n" :results none :sync
kubectl create secret generic ghost-passwords -n ghost \
    --from-literal=ghost-password=$GHOST_PASSWORD \
    --from-literal=smtp-password=$GHOST_SMTP_PASSWORD \
    --from-literal=mysql-root-password=$GHOST_MYSQL_ROOT_PASSWORD \
    --from-literal=mysql-password=$GHOST_MYSQL_ROOT_PASSWORD \
    -o yaml --dry-run=client > secrets/ghost.yaml
# kubectl apply -f ./ghost-passwords.yaml
#+end_src
** encrypt and commit ghost secret
We need to encrypt the pdns-secret with sops and commit/push so flux can decrypt and apply it
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :results silent :sync
sops --encrypt --in-place secrets/ghost.yaml
#+end_src
** abcs.news-tls
Need to setup another issuer over here... maybe the http one?
#+begin_src shell :results silent
kubectl --kubeconfig=$HOME/.kube/config-sharing.io -n ghost get secrets abcs.news-tls -o yaml > secrets/abcs.news-tls.yaml
#+end_src

** encrypt and commit tls-secret
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :results silent :sync
sops --encrypt --encrypted-regex '^(data|stringData)$' --in-place secrets/abcs.news-tls.yaml
#+end_src
** copy abcs.news-tls from old to new cluster
Need to setup another issuer over here... maybe the http one?
#+begin_src shell :results silent
kubectl --kubeconfig=$HOME/.kube/config-sharing.io -n ghost get secrets abcs.news-tls -o yaml | kubectl --kubeconfig=$HOME/.kube/config-uk.sharing.io apply -f -
#+end_src
** pvcs

Looks like we need to ensure *mount.nfs* is available from nfs-common.

#+begin_example
Events:
  Type     Reason                  Age                 From                     Message
  ----     ------                  ----                ----                     -------
  Normal   Scheduled               23m                 default-scheduler        Successfully assigned ghost/ghost-8499d5fb57-ncqqg to cp1.uk.sharing.io
  Normal   SuccessfulAttachVolume  23m                 attachdetach-controller  AttachVolume.Attach succeeded for volume "pvc-17166eda-0c12-49fe-b264-010dde15a5a3"
  Warning  FailedMount             37s (x19 over 23m)  kubelet                  MountVolume.MountDevice failed for volume "pvc-17166eda-0c12-49fe-b264-010dde15a5a3" : rpc error: code = Internal desc = mount failed: exit status 32
Mounting command: /usr/local/sbin/nsmounter
Mounting arguments: mount -t nfs -o vers=4.1,noresvport,intr,hard 10.98.159.230:/pvc-17166eda-0c12-49fe-b264-010dde15a5a3 /var/lib/kubelet/plugins/kubernetes.io/csi/driver.longhorn.io/fd606d5d5795c1a7eed567e3f011db4aa5f8b475f484e1120cc6af0556f54454/globalmount
Output: mount: /var/lib/kubelet/plugins/kubernetes.io/csi/driver.longhorn.io/fd606d5d5795c1a7eed567e3f011db4aa5f8b475f484e1120cc6af0556f54454/globalmount: bad option; for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
#+end_example

* harbor
https://goharbor.io/docs/1.10/administration/configure-authentication/oidc-auth/

https://harbor.uk.sharing.io/accounts/sign-in
* test
#+begin_src tmux
echo
#+end_src
* Footnotes
** LOB
*** defun eval-block
#+begin_src elisp :noweb yes
(defun eval-block (code-block)
  <<eval-block>>
  )
#+end_src

#+RESULTS:
: eval-block

*** eval-block
useful when wanting to asssign results of one block to variables of another
#+name: eval-block
#+begin_src elisp :var code-block="src name" :results silent
(save-excursion (org-babel-goto-named-src-block code-block)
  (let (
        (info (org-babel-get-src-block-info 'light))
        (re-run 't)
        )
    (when info
      (save-excursion
    ;; go to the results, if there aren't any then run the block
    (goto-char (or (and (not re-run) (org-babel-where-is-src-block-result))
               (progn (org-babel-execute-src-block)
                  (org-babel-where-is-src-block-result))))
    (end-of-line 1)
    (while (looking-at "[\n\r\t\f ]")
      (forward-char 1))
    ;; open the results
    (if (looking-at org-link-bracket-re)
        ;; file results
        (org-open-at-point)
      (let ((r (org-babel-format-result
            (org-babel-read-result) (cdr (assq :sep (nth 2 info))))))
        (print! "buffers: %s" (buffer-list))
        (pop-to-buffer (get-buffer-create "*Org-Babel Error*"))
        (print! "error content: %s" (buffer-string))
        (pop-to-buffer (get-buffer-create "load"))
        (print! "Messages content: %s" (buffer-string))
        ;; (pop-to-buffer (get-buffer-create "*Org-Babel Results*"))
        (delete-region (point-min) (point-max))
        (insert r)
        (setq results (buffer-string))))
    (string-trim-right results)))
    ))
;; (ob-eval-block code-block)
#+end_src
** PDNS TSIG Secret
https://github.com/zachomedia/cert-manager-webhook-pdns#powerdns-cert-manager-acme-webhook
Create one here with only access to uk.sharing.io
https://powerdns.ii.nz/admin/manage-keys
*** cert-manager
**** create rfc2136 secret
Note that the TSIG_KEY we retrieve is base64 encoded... it get's double encoded as a kubernetes secret. Most places you use a TSIG_KEY are expecting the base64 value we have here.
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :sync :results silent
source .envrc
kubectl -n cert-manager \
    create secret generic rfc2136 \
    --from-literal="$PDNS_TSIG_KEYNAME"="$PDNS_TSIG_KEY" \
    --dry-run=client -o yaml > \
     ./secrets/cert-manager-rfc2136.yaml
#+end_src

**** encrypt rfc2136 secret
We need to encrypt the pdns-secret with sops and commit/push so flux can decrypt and apply it
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :results silent :sync
sops --encrypt --in-place ./secrets/cert-manager-rfc2136.yaml
#+end_src

*** external-dns
**** create rfc2136 secret
Note that the TSIG_KEY we retrieve is base64 encoded... it get's double encoded as a kubernetes secret. Most places you use a TSIG_KEY are expecting the base64 value we have here.
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :sync :results silent
source .envrc
kubectl -n external-dns \
    create secret generic rfc2136 \
    --from-literal="$PDNS_TSIG_KEYNAME"="$PDNS_TSIG_KEY" \
    --dry-run=client -o yaml > \
    ./secrets/external-dns-rfc2136.yaml
#+end_src

**** encrypt rfc2136 secret
We need to encrypt the pdns-secret with sops and commit/push so flux can decrypt and apply it
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :results silent :sync
sops --encrypt --in-place ./secrets/external-dns-rfc2136.yaml
#+end_src

** Setup SOPS + Flux
*** sops binary
**** linux
#+begin_src shell
wget https://github.com/getsops/sops/releases/download/v3.7.3/sops_3.7.3_amd64.deb
dpkg -i sops_*deb
rm sops_*deb
#+end_src
**** mac
#+begin_src shell
brew install gnupg sops
#+end_src
*** generate gpg key
#+begin_src shell :env KEY_NAME="k8s.uk.sharing.io"
export KEY_NAME="k8s.uk.sharing.io"
export KEY_COMMENT="flux secrets"

gpg --batch --full-generate-key <<EOF
%no-protection
Key-Type: 1
Key-Length: 4096
Subkey-Type: 1
Subkey-Length: 4096
Expire-Date: 0
Name-Comment: ${KEY_COMMENT}
Name-Real: ${KEY_NAME}
EOF
#+end_src

#+begin_src shell :env KEY_NAME="k8s.uk.sharing.io"
export KEY_NAME="k8s.uk.sharing.io"
gpg --list-secret-keys "${KEY_NAME}"
#+end_src

#+end_example
*** import into kubernetes
#+begin_src shell :env KEY_NAME="k8s.uk.sharing.io"
export KEY_NAME="k8s.uk.sharing.io"
export KEY_FP=$(gpg --list-secret-keys "${KEY_NAME}" | grep SCEAR -A1 | tail -1 | awk '{print $1}')
gpg --export-secret-keys --armor "${KEY_FP}" |
kubectl create secret generic sops-gpg \
--namespace=flux-system \
--from-file=sops.asc=/dev/stdin
#+end_src

#+RESULTS:
#+begin_example
secret/sops-gpg created
#+end_example
*** export key into git
#+begin_src shell :env KEY_NAME="k8s.uk.sharing.io" :results silent
export KEY_NAME="k8s.uk.sharing.io"
export KEY_FP=$(gpg --list-secret-keys "${KEY_NAME}" | grep SCEAR -A1 | tail -1 | awk '{print $1}')
gpg --export --armor "${KEY_FP}" > ./.sops.pub.asc
#+end_src

*** write SOPS config file
#+begin_src shell :env KEY_NAME="k8s.uk.sharing.io" :results silent
export KEY_NAME="k8s.uk.sharing.io"
export KEY_FP=$(gpg --list-secret-keys "${KEY_NAME}" | grep SCEAR -A1 | tail -1 | awk '{print $1}')
cat <<EOF > .sops.yaml
creation_rules:
  - path_regex: .*.yaml
    encrypted_regex: ^(data|stringData)$
    pgp: ${KEY_FP}
EOF
#+end_src

https://fluxcd.io/flux/guides/mozilla-sops/#prerequisites
** Create .envrc
*** Example
#+begin_src shell :tangle .envrc.example :tangle-mode (identity #o600)
# Just needs to be a secrets `pwgen 12` is fine
export CODER_DB_PASSWORD=
export CODER_PG_CONNECTION_URL=postgres://postgres:$CODER_DB_PASSWORD@coder-db-postgres.coder.svc.cluster.local:5432/coder?sslmode=disabled
# We Could set a GITHUB Token for coder, but not for now
# https://github.com/settings/personal-access-tokens/new
# https://github.com/settings/personal-access-tokens/1566846
export GITHUB_TOKEN=
# https://coder.com/docs/v2/latest/admin/auth
# https://github.com/organizations/cloudnative-nz/settings/applications/new
# https://github.com/organizations/cloudnative-nz/settings/applications/2247328
export CODER_OAUTH2_GITHUB_CLIENT_ID=
export CODER_OAUTH2_GITHUB_CLIENT_SECRET=
# https://coder.com/docs/v2/latest/admin/git-providers
# https://github.com/organizations/cloudnative-nz/settings/apps/new
# https://github.com/organizations/cloudnative-nz/settings/apps/space-cloudnative-nz
# NOTE: this is NOT the App ID, use Client ID
export CODER_GITAUTH_0_CLIENT_ID=
# NOTE: this is Client Secret
export CODER_GITAUTH_0_CLIENT_SECRET=
# We created a tsig key with DNS-UPDATE only for uk.sharing.io
export PDNS_API_KEY=
# I created a new project a key
# https://console.equinix.com/projects/0c218738-18c0-47b5-a404-beb59d9c6585/general
export METAL_PROJECT_ID=
# https://console.equinix.com/projects/0c218738-18c0-47b5-a404-beb59d9c6585/api-keys
export METAL_AUTH_TOKEN=
# OIDC with authentik deployed to sso.ii.nz
# https://sso.ii.nz/if/admin/#/core/applications/cloudnative-nz
# https://sso.ii.nz/if/admin/#/core/providers/2
export CODER_OIDC_CLIENT_ID=
export CODER_OIDC_CLIENT_SECRET=
#+end_src

#+RESULTS:
: 0f4a4ae48da28e6d60a5f9e7df0d1c02

*** Save .envrc to .enc.envrc with sops
#+begin_src tmux :session "cluster:secret"
sops -e --input-type dotenv .envrc > .enc.envrc
#+end_src
** Create Kubernetes Secrets
*** Save pdns TSIG key as a kubernetes secret
PDNS_TSIG_KEY needs to be set to the activated TSIG key with TSIG-ALLOW_DNSUPDATE
#+begin_src tmux :session "cluster:secret"
. .envrc
echo $PDNS_TSIG_KEY
kubectl create secret generic pdns \
    --namespace=cert-manager \
    --from-literal=key=$PDNS_TSIG_KEY \
    -o yaml \
    --dry-run=client > ./pdns-secret.yaml
sops --encrypt --in-place pdns-secret.yaml
#+end_src

*** generate coder secret
Mostly we map the env vars directly.
#+begin_src tmux
cd ~/sharingio/infra/clusters/container.sharing.io
. .envrc
kubectl create secret generic coder \
    --namespace=coder \
    --from-literal=TUNNELD_WIREGUARD_KEY=$TUNNELD_WIREGUARD_KEY \
    --from-literal=PDNS_TSIG_KEY=$PDNS_TSIG_KEY \
    --from-literal=PDNS_API_KEY=$PDNS_API_KEY \
    --from-literal=GITHUB_TOKEN=$GITHUB_TOKEN \
    --from-literal=CODER_USERNAME=$CODER_USERNAME \
    --from-literal=CODER_PASSWORD=$CODER_PASSWORD \
    --from-literal=CODER_OAUTH2_GITHUB_CLIENT_ID=$CODER_OAUTH2_GITHUB_CLIENT_ID \
    --from-literal=CODER_OAUTH2_GITHUB_CLIENT_SECRET=$CODER_OAUTH2_GITHUB_CLIENT_SECRET \
    --from-literal=CODER_GITAUTH_0_CLIENT_ID=$CODER_GITAUTH_0_CLIENT_ID \
    --from-literal=CODER_GITAUTH_0_CLIENT_SECRET=$CODER_GITAUTH_0_CLIENT_SECRET \
    --from-literal=CODER_OIDC_CLIENT_ID=$CODER_OIDC_CLIENT_ID \
    --from-literal=CODER_OIDC_CLIENT_SECRET=$CODER_OIDC_CLIENT_SECRET \
    --from-literal=METAL_AUTH_TOKEN=$METAL_AUTH_TOKEN \
    --from-literal=password=$CODER_DB_PASSWORD \
    --from-literal=postgres-password=$CODER_DB_PASSWORD \
    --from-literal=CODER_PG_CONNECTION_URL="postgres://postgres:$CODER_DB_PASSWORD@coder-db-postgresql.coder.svc.cluster.local:5432/coder?sslmode=disable" \
    -o yaml --dry-run=client > ./secrets/coder.yaml
#+end_src

*** encode coder secret
Mostly we map the env vars directly.
#+begin_src tmux
sops --encrypt --in-place ./secrets/coder.yaml
#+end_src
*** describe coder secret
Mostly we map the env vars directly.
#+begin_src bash :sync :dir .
kubectl -n coder describe secret coder
#+end_src

#+RESULTS:
#+begin_example
Name:         coder
Namespace:    coder
Labels:       kustomize.toolkit.fluxcd.io/name=secrets
              kustomize.toolkit.fluxcd.io/namespace=flux-system
Annotations:  <none>

Type:  Opaque

Data
====
CODER_OIDC_CLIENT_ID:               40 bytes
GITHUB_TOKEN:                       93 bytes
METAL_AUTH_TOKEN:                   32 bytes
PDNS_API_KEY:                       8 bytes
CODER_GITAUTH_0_CLIENT_ID:          20 bytes
CODER_GITAUTH_0_CLIENT_SECRET:      40 bytes
CODER_OAUTH2_GITHUB_CLIENT_ID:      20 bytes
CODER_OAUTH2_GITHUB_CLIENT_SECRET:  40 bytes
postgres-password:                  12 bytes
CODER_OIDC_CLIENT_SECRET:           128 bytes
CODER_PG_CONNECTION_URL:            103 bytes
PDNS_TSIG_KEY:                      88 bytes
password:                           12 bytes
#+end_example

** Variables
# Local Variables:
# space-domain: "container.sharing.io"
# tramp-dir: "/ssh:root@136.144.49.79:/"
# eval: (setq org-babel-tmux-terminal "kitty")
# eval: (setq org-babel-tmux-terminal-opts '("--hold"))
# End:
